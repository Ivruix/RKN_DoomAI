{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9850688,"sourceType":"datasetVersion","datasetId":6044405}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install annoy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\nimport torch\nfrom transformers import CLIPProcessor, CLIPModel\nfrom annoy import AnnoyIndex","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Задаем параметры Annoy\nembedding_dim = 512  # Размерность эмбеддинга\nindex = AnnoyIndex(embedding_dim, 'angular')  # Используем косинусное расстояние для Annoy\n\n# Загружаем модель CLIP\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# Указываем путь к корневой директории и файлу для сохранения метаданных\nroot_directory = \"/kaggle/input/rkn-dataset/train_dataset_train_data_rkn/train_data_rkn/dataset\"\nmetadata_file = \"annoy_metadata.json\"","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Создаем словарь для хранения метаданных\nmetadata = {}\nindex_counter = 0\n\n# Проход по всем файлам в поддиректориях\nfor dirpath, _, filenames in tqdm_notebook(os.walk(root_directory)):\n    for filename in filenames:\n        # Проверка расширения файла (если файл является изображением)\n        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n            file_path = os.path.join(dirpath, filename)\n            dir_name = os.path.basename(dirpath)  # Название поддиректории\n            try:\n                # Открываем изображение\n                image = Image.open(file_path).convert(\"RGB\")\n\n                # Преобразуем изображение в вектор с помощью CLIP\n                inputs = processor(images=image, return_tensors=\"pt\")\n                with torch.no_grad():\n                    image_embedding = model.get_image_features(**inputs)\n\n                # Конвертируем тензор в numpy для совместимости с Annoy\n                image_embedding = image_embedding.cpu().numpy().flatten()\n\n                # Добавляем вектор в индекс Annoy\n                index.add_item(index_counter, image_embedding)\n\n                # Сохраняем метаданные\n                metadata[index_counter] = {\n                    \"filename\": filename,\n                    \"directory\": dir_name\n                }\n                index_counter += 1\n\n            except Exception as e:\n                print(f\"Ошибка обработки изображения {file_path}: {e}\")\n\n# Строим индекс Annoy с 10 деревьями (параметр можно регулировать для повышения точности)\nindex.build(10)\n\n# Сохраняем Annoy-индекс и метаданные\nindex.save(\"image_embeddings.ann\")\nwith open(metadata_file, 'w') as f:\n    json.dump(metadata, f)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}